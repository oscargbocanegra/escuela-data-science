{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08cdd2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando las librerias para el ejercicio.\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType\n",
    "from pyspark.sql.types import Row\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97deebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el contexto que funciona como punto de conexion para todo spark.\n",
    "spark = SparkContext(master=\"local\", appName=\"DataFrames\")\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fab50ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando una variable que guarde la ruta de los archivos de datos.\n",
    "#path = \"/home/jovyan/work/files/\"\n",
    "path = \"/Users/giova/OneDrive/Escritorio/Curses/platzi/fundamentos-spark-big-data/files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86773b95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Consultando la ruta para ubicar los archivos.\n",
    "#!ls /home/jovyan/work/files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d925d82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+----+---------+---------+\n",
      "|juego_id|nombre_juego|anio|temporada|   ciudad|\n",
      "+--------+------------+----+---------+---------+\n",
      "|       1| 1896 Verano|1896|   Verano|   Athina|\n",
      "|       2| 1900 Verano|1900|   Verano|    Paris|\n",
      "|       3| 1904 Verano|1904|   Verano|St. Louis|\n",
      "|       4| 1906 Verano|1906|   Verano|   Athina|\n",
      "+--------+------------+----+---------+---------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creando Schema para los campos del RDD.\n",
    "Schema_juegosRDD = StructType([\n",
    "    StructField(\"juego_id\", IntegerType(),False),\n",
    "    StructField(\"nombre_juego\", StringType(),False),\n",
    "    StructField(\"anio\", IntegerType(),False),\n",
    "    StructField(\"temporada\", StringType(),False),\n",
    "    StructField(\"ciudad\", StringType(),False)\n",
    "])\n",
    "\n",
    "\n",
    "# Indicando lectura para cargar el archivo.\n",
    "juegoDF = sqlContext.read.schema(Schema_juegosRDD).option(\"header\",\"true\").csv(path + \"juegos.csv\")\n",
    "\n",
    "# Visualizando el dataframe con show().\n",
    "juegoDF.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7613acad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando DataFrames de archivos .data\n",
    "autoDataRDD = spark.textFile(path+\"auto-mpg.data\").map(lambda l : l.replace(\"\\\\s\",\"\"))\n",
    "imports85RDD = spark.textFile(path+\"imports-85.data\").map(lambda l : l.replace(\"\\\\s\",\"\"))\n",
    "synthetic_controlRDD = spark.textFile(path+\"synthetic_control.data\").map(lambda l : l.replace(\"\\\\s\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52a33233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- juego_id: integer (nullable = true)\n",
      " |-- nombre_juego: string (nullable = true)\n",
      " |-- anio: integer (nullable = true)\n",
      " |-- temporada: string (nullable = true)\n",
      " |-- ciudad: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Consultando el schema de un Dataframe.\n",
    "juegoDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6838d59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando funcion para eliminar encabezados\n",
    "def eliminaEncabezado(indice, iterador):\n",
    "    return iter(list(iterador)[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdcec328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+------+----+------+----+---------+\n",
      "|deportista_id|              nombre|genero|edad|altura|peso|equipo_id|\n",
      "+-------------+--------------------+------+----+------+----+---------+\n",
      "|            1|           A Dijiang|     1|  24|   180|80.0|      199|\n",
      "|            2|            A Lamusi|     1|  23|   170|60.0|      199|\n",
      "|            3| Gunnar Nielsen Aaby|     1|  24|     0| 0.0|      273|\n",
      "|            4|Edgar Lindenau Aabye|     1|  34|     0| 0.0|      278|\n",
      "+-------------+--------------------+------+----+------+----+---------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creando el Dataframes a partir de los RDD correspondiente al archivo deportista.csv.\n",
    "deportistaOlimpicoRDD = spark.textFile(path+\"deportista.csv\").map(lambda l : l.split(\",\"))\n",
    "\n",
    "# Pasando El RDD a la funcion creada para eliminar el encabezado.\n",
    "deportistaOlimpicoRDD = deportistaOlimpicoRDD.mapPartitionsWithIndex(eliminaEncabezado)\n",
    "\n",
    "# Transformando los valores del RDD.\n",
    "deportistaOlimpicoRDD = deportistaOlimpicoRDD.map(lambda l : (\n",
    "int(l[0]),\n",
    "l[1],\n",
    "int(l[2]),\n",
    "int(l[3]),\n",
    "int(l[4]),\n",
    "float(l[5]),\n",
    "int(l[6])\n",
    "))\n",
    "\n",
    "# Creando el Schema para los campos del RDD\n",
    "schemadeportistaOlimpicoRDD = StructType([\n",
    "    StructField(\"deportista_id\",IntegerType(),False),\n",
    "    StructField(\"nombre\",StringType(),False),\n",
    "    StructField(\"genero\",IntegerType(),False),\n",
    "    StructField(\"edad\",IntegerType(),False),\n",
    "    StructField(\"altura\",IntegerType(),False),\n",
    "    StructField(\"peso\",FloatType(),False),\n",
    "    StructField(\"equipo_id\",IntegerType(),False)\n",
    "])\n",
    "\n",
    "# Visualizando la transformación del Dataframe deportistaDF.\n",
    "#deportistaDF = sqlContext.createDataFrame(deportistaOlimpicoRDD,schemadeportistaOlimpicoRDD)\n",
    "deportistaOlimpicoDF = sqlContext.read.schema(schemadeportistaOlimpicoRDD).option(\"header\",\"true\").csv(path + \"deportista.csv\")\n",
    "\n",
    "# Visualizando el dataframe con show().\n",
    "deportistaOlimpicoDF.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3a419e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el Dataframes a partir de los RDD correspondiente al archivo data.csv\n",
    "dataRDD = spark.textFile(path+\"data.csv\").map(lambda l : l.split(\",\"))\n",
    "\n",
    "# Pasando El RDD a la funcion creada para eliminar el encabezado.\n",
    "dataRDD = dataRDD.mapPartitionsWithIndex(eliminaEncabezado)\n",
    "\n",
    "# Transformando los valores del RDD.\n",
    "dataRDD = dataRDD.map(lambda l : (\n",
    "l[0],\n",
    "l[1],\n",
    "int(l[2])\n",
    "))\n",
    "\n",
    "# Creando Schema para los campos del RDD.\n",
    "schemadataRDD = StructType([\n",
    "    StructField(\"Estado\",StringType(),False),\n",
    "    StructField(\"color\",StringType(),False),\n",
    "    StructField(\"contador\",IntegerType(),False)\n",
    "])\n",
    "\n",
    "# Visualizando la transformación del Dataframe.\n",
    "#dataDF = sqlContext.createDataFrame(dataRDD,schemadataRDD)\n",
    "dataDF = sqlContext.read.schema(schemadataRDD).option(\"header\",\"true\").csv(path + \"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b96bcc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+\n",
      "|Estado|color|contador|\n",
      "+------+-----+--------+\n",
      "|    TX|  Red|      20|\n",
      "|    NV| Blue|      66|\n",
      "|    CO| Blue|      79|\n",
      "|    OR| Blue|      71|\n",
      "+------+-----+--------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00ba182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el Dataframes a partir de los RDD correspondiente al archivo deporte.csv\n",
    "deporteRDD = spark.textFile(path+\"deporte.csv\").map(lambda l : l.split(\",\"))\n",
    "\n",
    "# Pasando El RDD a la funcion creada para eliminar el encabezado.\n",
    "deporteRDD = deporteRDD.mapPartitionsWithIndex(eliminaEncabezado)\n",
    "\n",
    "# Transformando los valores del RDD.\n",
    "deporteRDD = deporteRDD.map(lambda l : (\n",
    "int(l[0]),\n",
    "l[1]\n",
    "))\n",
    "\n",
    "# Creando Schema para los campos del RDD.\n",
    "schema_deporteRDD = StructType([\n",
    "    StructField(\"deporte_Id\",IntegerType(),False),\n",
    "    StructField(\"deporte\",StringType(),False)\n",
    "])\n",
    "\n",
    "# Visualizando la transformación del Dataframe.\n",
    "#deporteDF = sqlContext.createDataFrame(deporteRDD,schema_deporteRDD)\n",
    "deporteDF = sqlContext.read.schema(schema_deporteRDD).option(\"header\",\"true\").csv(path + \"deporte.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "948f07b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|deporte_Id|   deporte|\n",
      "+----------+----------+\n",
      "|         1|Basketball|\n",
      "|         2|      Judo|\n",
      "|         3|  Football|\n",
      "|         4|Tug-Of-War|\n",
      "+----------+----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deporteDF.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16ba151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el Dataframes a partir de los RDD correspondiente al archivo deportista2.csv\n",
    "deportista2RDD = spark.textFile(path+\"deportista2.csv\").map(lambda l : l.split(\",\"))\n",
    "\n",
    "# Pasando El RDD a la funcion creada para eliminar el encabezado.\n",
    "deportista2RDD = deportista2RDD.mapPartitionsWithIndex(eliminaEncabezado)\n",
    "\n",
    "# Transformando los valores del RDD.\n",
    "deportista2RDD = deportista2RDD.map(lambda l : (\n",
    "int(l[0]),\n",
    "l[1],\n",
    "int(l[2]),\n",
    "int(l[3]),\n",
    "int(l[4]),\n",
    "float(l[5]),\n",
    "int(l[6])\n",
    "))\n",
    "\n",
    "# Creando Schema para los campos del RDD.\n",
    "schema_deportista2RDD = StructType([\n",
    "    StructField(\"deportista_id\",IntegerType(),False),\n",
    "    StructField(\"nombre\",StringType(),False),\n",
    "    StructField(\"genero\",IntegerType(),False),\n",
    "    StructField(\"edad\",IntegerType(),False),\n",
    "    StructField(\"altura\",IntegerType(),False),\n",
    "    StructField(\"peso\",FloatType(),False),\n",
    "    StructField(\"equipo_id\",IntegerType(),False)\n",
    "])\n",
    "\n",
    "# Visualizando la transformación del Dataframe.\n",
    "#deportista2DF = sqlContext.createDataFrame(deportista2RDD,schema_deportista2RDD)\n",
    "deportista2DF = sqlContext.read.schema(schema_deportista2RDD).option(\"header\",\"true\").csv(path + \"deportista2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f96dc3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+------+----+------+----+---------+\n",
      "|deportista_id|              nombre|genero|edad|altura|peso|equipo_id|\n",
      "+-------------+--------------------+------+----+------+----+---------+\n",
      "|        67788|            Lee BuTi|     1|  23|   164|54.0|      203|\n",
      "|        67789|Anthony N. Buddy Lee|     1|  34|   172|62.0|     1096|\n",
      "|        67790|Alfred A. Butch L...|     1|  19|   186|80.0|      825|\n",
      "|        67791|        Lee ByeongGu|     1|  22|   175|68.0|      970|\n",
      "+-------------+--------------------+------+----+------+----+---------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deportista2DF.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9e61917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el Dataframes a partir de los RDD correspondiente al archivo evento.csv\n",
    "eventoRDD = spark.textFile(path+\"evento.csv\").map(lambda l : l.split(\",\"))\n",
    "\n",
    "# Pasando El RDD a la funcion creada para eliminar el encabezado.\n",
    "eventoRDD = eventoRDD.mapPartitionsWithIndex(eliminaEncabezado)\n",
    "\n",
    "# Transformando los valores del RDD.\n",
    "eventoRDD = eventoRDD.map(lambda l : (\n",
    "int(l[0]),\n",
    "l[1],\n",
    "l[2]\n",
    "))\n",
    "\n",
    "# Creando Schema para los campos del RDD.\n",
    "schema_eventoRDD = StructType([\n",
    "    StructField(\"evento_id\",IntegerType(),False),\n",
    "    StructField(\"tipo_evento\",StringType(),False),\n",
    "    StructField(\"id_deporte\",StringType(),False)\n",
    "])\n",
    "# Visualizando la transformación del Dataframe.\n",
    "#eventoRDD = sqlContext.createDataFrame(eventoRDD,schema_eventoRDD)\n",
    "deportesOlimpicosDF = sqlContext.read.schema(schema_eventoRDD).option(\"header\",\"true\").csv(path + \"evento.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cad3ec59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+----------+\n",
      "|evento_id|         tipo_evento|id_deporte|\n",
      "+---------+--------------------+----------+\n",
      "|        1|Basketball Men's ...|         1|\n",
      "|        2|Judo Men's Extra-...|         2|\n",
      "|        3|Football Men's Fo...|         3|\n",
      "|        4|Tug-Of-War Men's ...|         4|\n",
      "+---------+--------------------+----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deportesOlimpicosDF.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62648569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el Dataframes a partir de los RDD correspondiente al archivo juegos.csv\n",
    "juegosRDD = spark.textFile(path+\"juegos.csv\").map(lambda l : l.split(\",\"))\n",
    "\n",
    "# Pasando El RDD a la funcion creada para eliminar el encabezado\n",
    "juegosRDD = juegosRDD.mapPartitionsWithIndex(eliminaEncabezado)\n",
    "\n",
    "# Transformando los valores del RDD.\n",
    "juegosRDD = juegosRDD.map(lambda l : (\n",
    "int(l[0]),\n",
    "l[1],\n",
    "int(l[2]),\n",
    "l[3],\n",
    "l[4]\n",
    "))\n",
    "\n",
    "# Creando Schema para los campos del RDD.\n",
    "Schema_juegosRDD = StructType([\n",
    "    StructField(\"juego_id\", IntegerType(),False),\n",
    "    StructField(\"nombre_juego\", StringType(),False),\n",
    "    StructField(\"anio\", IntegerType(),False),\n",
    "    StructField(\"temporada\", StringType(),False),\n",
    "    StructField(\"ciudad\", StringType(),False)\n",
    "])\n",
    "\n",
    "# Visualizando la transformación del Dataframe.\n",
    "#juegosRDD = sqlContext.createDataFrame(juegosRDD,Schema_juegosRDD)\n",
    "juegosDF = sqlContext.read.schema(Schema_juegosRDD).option(\"header\",\"true\").csv(path + \"juegos.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42bde994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+----+---------+---------+\n",
      "|juego_id|nombre_juego|anio|temporada|   ciudad|\n",
      "+--------+------------+----+---------+---------+\n",
      "|       1| 1896 Verano|1896|   Verano|   Athina|\n",
      "|       2| 1900 Verano|1900|   Verano|    Paris|\n",
      "|       3| 1904 Verano|1904|   Verano|St. Louis|\n",
      "|       4| 1906 Verano|1906|   Verano|   Athina|\n",
      "+--------+------------+----+---------+---------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "juegosDF.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42fa47e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el Dataframes a partir de los RDD correspondiente al archivo paises.csv\n",
    "paisesRDD = spark.textFile(path+\"paises.csv\").map(lambda l : l.split(\",\"))\n",
    "\n",
    "# Pasando El RDD a la funcion creada para eliminar el encabezado\n",
    "paisesRDD = paisesRDD.mapPartitionsWithIndex(eliminaEncabezado)\n",
    "\n",
    "# Transformando los valores del RDD.\n",
    "paisesRDD = paisesRDD.map(lambda l : (\n",
    "int(l[0]),\n",
    "l[1],\n",
    "l[2]\n",
    "))\n",
    "\n",
    "# Creando Schema para los campos del RDD.\n",
    "Schema_paisesRDD = StructType([\n",
    "    StructField(\"pais_id\", IntegerType(),False),\n",
    "    StructField(\"nombre_equipo\", StringType(),False),\n",
    "    StructField(\"sigla_equipo\", StringType(),False)\n",
    "])\n",
    "\n",
    "# Visualizando la transformación del Dataframe.\n",
    "#paisesRDD = sqlContext.createDataFrame(paisesRDD,Schema_paisesRDD)\n",
    "paisesDF = sqlContext.read.schema(Schema_paisesRDD).option(\"header\",\"true\").csv(path + \"paises.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f61a4cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------+\n",
      "|pais_id|       nombre_equipo|sigla_equipo|\n",
      "+-------+--------------------+------------+\n",
      "|      1|         30. Februar|         AUT|\n",
      "|      2|A North American ...|         MEX|\n",
      "|      3|           Acipactli|         MEX|\n",
      "|      4|             Acturus|         ARG|\n",
      "+-------+--------------------+------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paisesDF.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58abc91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el Dataframes a partir de los RDD correspondiente al archivo resultados.csv\n",
    "resultadosRDD = spark.textFile(path+\"resultados.csv\").map(lambda l : l.split(\",\"))\n",
    "\n",
    "# Pasando El RDD a la funcion creada para eliminar el encabezado\n",
    "resultadosRDD = resultadosRDD.mapPartitionsWithIndex(eliminaEncabezado)\n",
    "\n",
    "# Transformando los valores del RDD.\n",
    "resultadosRDD = resultadosRDD.map(lambda l : (\n",
    "int(l[0]),\n",
    "l[1],\n",
    "int(l[2]),\n",
    "int(l[3]),\n",
    "int(l[4])\n",
    "))\n",
    "\n",
    "# Creando Schema para los campos del RDD.\n",
    "schema_resultadosRDD = StructType([\n",
    "    StructField(\"resultado_id\",IntegerType(),False),\n",
    "    StructField(\"medalla\",StringType(),False),\n",
    "    StructField(\"deportista_id\",IntegerType(),False),\n",
    "    StructField(\"juego_id\",IntegerType(),False),\n",
    "    StructField(\"evento_id\",IntegerType(),False)\n",
    "])\n",
    "\n",
    "# Visualizando la transformación del Dataframe.\n",
    "#resultadosRDD = sqlContext.createDataFrame(resultadosRDD,schema_resultadosRDD)\n",
    "resultadosDF = sqlContext.read.schema(schema_resultadosRDD).option(\"header\",\"true\").csv(path + \"resultados.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f4b55cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------------+--------+---------+\n",
      "|resultado_id|medalla|deportista_id|juego_id|evento_id|\n",
      "+------------+-------+-------------+--------+---------+\n",
      "|           1|     NA|            1|      39|        1|\n",
      "|           2|     NA|            2|      49|        2|\n",
      "|           3|     NA|            3|       7|        3|\n",
      "|           4|   Gold|            4|       2|        4|\n",
      "+------------+-------+-------------+--------+---------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultadosDF.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9860d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- resultado_id: integer (nullable = true)\n",
      " |-- medalla: string (nullable = true)\n",
      " |-- deportista_id: integer (nullable = true)\n",
      " |-- juego_id: integer (nullable = true)\n",
      " |-- evento_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# La funcion printschema() permite visualizar el esquema que tiene el DF.\n",
    "resultadosDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ded99046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La funcion withColumnRenamed(\"\",\"\").drop(\"\") permite borrar o renombrar columnas.\n",
    "#resultadosDF = resultadosDF.withColumnRenamed(\"medalla\", \"medallota\")\n",
    "#resultadosDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d249702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generando consultas con funcion select\n",
    "#deportistaOlimpicoDF = deportistaOlimpicoDF.select(\"deportista_id\", \"nombre\", col(\"edad\").alias(\"edadAlJugar\"),\"equipo_id\")\n",
    "#deportistaOlimpicoDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d7481c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generando consultas con funcion filter\n",
    "#deportistaOlimpicoDF = deportistaOlimpicoDF.filter(deportistaOlimpicoDF.edadAlJugar != 0)\n",
    "#deportistaOlimpicoDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "357b11de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consultando el deportista mas joven de los juegos olimpicos\n",
    "#deportistaOlimpicoDF.sort(\"edadAlJugar\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fbcad30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- resultado_id: integer (nullable = true)\n",
      " |-- medalla: string (nullable = true)\n",
      " |-- deportista_id: integer (nullable = true)\n",
      " |-- juego_id: integer (nullable = true)\n",
      " |-- evento_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultadosDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9cf674df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-------+------------+--------------------+\n",
      "|              nombre|Edad al jugar|medalla|Año de Juego|   Nombre disciplina|\n",
      "+--------------------+-------------+-------+------------+--------------------+\n",
      "|           A Dijiang|           24|     NA|        1992|Basketball Men's ...|\n",
      "|            A Lamusi|           23|     NA|        2012|Judo Men's Extra-...|\n",
      "| Gunnar Nielsen Aaby|           24|     NA|        1920|Football Men's Fo...|\n",
      "|Edgar Lindenau Aabye|           34|   Gold|        1900|Tug-Of-War Men's ...|\n",
      "|Christine Jacoba ...|           21|     NA|        1994|Speed Skating Wom...|\n",
      "|Christine Jacoba ...|           21|     NA|        1994|Speed Skating Wom...|\n",
      "|Christine Jacoba ...|           21|     NA|        1992|Speed Skating Wom...|\n",
      "|Christine Jacoba ...|           21|     NA|        1992|Speed Skating Wom...|\n",
      "|Christine Jacoba ...|           21|     NA|        1988|Speed Skating Wom...|\n",
      "|Christine Jacoba ...|           21|     NA|        1988|Speed Skating Wom...|\n",
      "|     Per Knut Aaland|           31|     NA|        1994|Cross Country Ski...|\n",
      "|     Per Knut Aaland|           31|     NA|        1994|Cross Country Ski...|\n",
      "|     Per Knut Aaland|           31|     NA|        1994|Cross Country Ski...|\n",
      "|     Per Knut Aaland|           31|     NA|        1994|Cross Country Ski...|\n",
      "|     Per Knut Aaland|           31|     NA|        1992|Cross Country Ski...|\n",
      "|     Per Knut Aaland|           31|     NA|        1992|Cross Country Ski...|\n",
      "|     Per Knut Aaland|           31|     NA|        1992|Cross Country Ski...|\n",
      "|     Per Knut Aaland|           31|     NA|        1992|Cross Country Ski...|\n",
      "|        John Aalberg|           31|     NA|        1994|Cross Country Ski...|\n",
      "|        John Aalberg|           31|     NA|        1994|Cross Country Ski...|\n",
      "+--------------------+-------------+-------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creando consultas usando Join para identificar los deportistas que participaron en las olimpiadas con sus disciplinas.\n",
    "deportistaOlimpicoDF.join(resultadosDF, deportistaOlimpicoDF.deportista_id == resultadosDF.deportista_id, \"left\") \\\n",
    "    .join(juegosDF, juegosDF.juego_id == resultadosDF.juego_id, \"left\") \\\n",
    "    .join(deportesOlimpicosDF, deportesOlimpicosDF.evento_id == resultadosDF.evento_id, \"left\") \\\n",
    "    .select(deportistaOlimpicoDF.nombre, col(\"edad\").alias(\"Edad al jugar\"), \n",
    "            \"medalla\", col(\"anio\").alias(\"Año de Juego\"), \n",
    "            deportesOlimpicosDF.tipo_evento.alias(\"Nombre disciplina\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aea4606b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+------------+\n",
      "|medalla|nombre_equipo|sigla_equipo|\n",
      "+-------+-------------+------------+\n",
      "| Silver|     Zimbabwe|         ZIM|\n",
      "| Silver|     Zimbabwe|         ZIM|\n",
      "|   Gold|     Zimbabwe|         ZIM|\n",
      "|   Gold|     Zimbabwe|         ZIM|\n",
      "|   Gold|     Zimbabwe|         ZIM|\n",
      "|   Gold|     Zimbabwe|         ZIM|\n",
      "|   Gold|     Zimbabwe|         ZIM|\n",
      "|   Gold|     Zimbabwe|         ZIM|\n",
      "|   Gold|     Zimbabwe|         ZIM|\n",
      "| Silver|     Zimbabwe|         ZIM|\n",
      "|   Gold|     Zimbabwe|         ZIM|\n",
      "| Silver|     Zimbabwe|         ZIM|\n",
      "|   Gold|     Zimbabwe|         ZIM|\n",
      "|   Gold|     Zimbabwe|         ZIM|\n",
      "| Bronze|     Zimbabwe|         ZIM|\n",
      "| Bronze|   Yugoslavia|         YUG|\n",
      "|   Gold|   Yugoslavia|         YUG|\n",
      "| Silver|   Yugoslavia|         YUG|\n",
      "| Silver|   Yugoslavia|         YUG|\n",
      "| Silver|   Yugoslavia|         YUG|\n",
      "+-------+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reto de consulta para obtener como resultado los deportistas\n",
    "resultadosDF.filter(resultadosDF.medalla != \"NA\") \\\n",
    "    .join(deportistaOlimpicoDF, deportistaOlimpicoDF.deportista_id == resultadosDF.deportista_id, \"left\") \\\n",
    "    .join(paisesDF, paisesDF.pais_id == deportistaOlimpicoDF.equipo_id, \"left\") \\\n",
    "    .select(\"medalla\", \"nombre_equipo\", \"sigla_equipo\") \\\n",
    "    .sort(col(\"sigla_equipo\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "22cabce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+-------+--------------------+--------------------+--------------------+\n",
      "|sigla_equipo|anio|medalla|Nombre subdisciplina|   Nombre disciplina|              nombre|\n",
      "+------------+----+-------+--------------------+--------------------+--------------------+\n",
      "|         CHN|1992|     NA|Basketball Men's ...|          Basketball|           A Dijiang|\n",
      "|         CHN|2012|     NA|Judo Men's Extra-...|                Judo|            A Lamusi|\n",
      "|         DEN|1920|     NA|Football Men's Fo...|            Football| Gunnar Nielsen Aaby|\n",
      "|         SWE|1900|   Gold|Tug-Of-War Men's ...|          Tug-Of-War|Edgar Lindenau Aabye|\n",
      "|         NED|1994|     NA|Speed Skating Wom...|       Speed Skating|Christine Jacoba ...|\n",
      "|         NED|1994|     NA|Speed Skating Wom...|       Speed Skating|Christine Jacoba ...|\n",
      "|         NED|1992|     NA|Speed Skating Wom...|       Speed Skating|Christine Jacoba ...|\n",
      "|         NED|1992|     NA|Speed Skating Wom...|       Speed Skating|Christine Jacoba ...|\n",
      "|         NED|1988|     NA|Speed Skating Wom...|       Speed Skating|Christine Jacoba ...|\n",
      "|         NED|1988|     NA|Speed Skating Wom...|       Speed Skating|Christine Jacoba ...|\n",
      "|         USA|1994|     NA|Cross Country Ski...|Cross Country Skiing|     Per Knut Aaland|\n",
      "|         USA|1994|     NA|Cross Country Ski...|Cross Country Skiing|     Per Knut Aaland|\n",
      "|         USA|1994|     NA|Cross Country Ski...|Cross Country Skiing|     Per Knut Aaland|\n",
      "|         USA|1994|     NA|Cross Country Ski...|Cross Country Skiing|     Per Knut Aaland|\n",
      "|         USA|1992|     NA|Cross Country Ski...|Cross Country Skiing|     Per Knut Aaland|\n",
      "|         USA|1992|     NA|Cross Country Ski...|Cross Country Skiing|     Per Knut Aaland|\n",
      "|         USA|1992|     NA|Cross Country Ski...|Cross Country Skiing|     Per Knut Aaland|\n",
      "|         USA|1992|     NA|Cross Country Ski...|Cross Country Skiing|     Per Knut Aaland|\n",
      "|         USA|1994|     NA|Cross Country Ski...|           Athletics|        John Aalberg|\n",
      "|         USA|1994|     NA|Cross Country Ski...|           Athletics|        John Aalberg|\n",
      "+------------+----+-------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Funciones de Agregación. reto hacer los respectivos cruces para obtener todos los medallistas que han jugado en las olimpiadas, \n",
    "# mostrar equipo, medalla, el nombre del jugador y la sub disiplina de competencia.\n",
    "\n",
    "medallistaXAnio = deportistaOlimpicoDF \\\n",
    "    .join(resultadosDF, deportistaOlimpicoDF.deportista_id == resultadosDF.deportista_id, \"left\") \\\n",
    "    .join(juegosDF, juegosDF.juego_id == resultadosDF.juego_id, \"left\") \\\n",
    "    .join(paisesDF, deportistaOlimpicoDF.equipo_id == paisesDF.pais_id, \"left\") \\\n",
    "    .join(deportesOlimpicosDF, deportesOlimpicosDF.evento_id == resultadosDF.evento_id, \"left\") \\\n",
    "    .join(deporteDF, deportistaOlimpicoDF.deportista_id == deporteDF.deporte_Id, \"left\") \\\n",
    "    .select(\"sigla_equipo\", \"anio\", \"medalla\", deportesOlimpicosDF.tipo_evento.alias(\"Nombre subdisciplina\"),\n",
    "        deporteDF.deporte.alias(\"Nombre disciplina\"), deportistaOlimpicoDF.nombre)\n",
    "\n",
    "medallistaXAnio.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb1e2417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando una agrupacion en el DF creado en el punto anterior y guardando resultado en medallistaXAnio2.\n",
    "medallistaXAnio2 = medallistaXAnio.filter(medallistaXAnio.medalla != \"NA\") \\\n",
    "    .sort(\"anio\") \\\n",
    "    .groupBy(\"sigla_equipo\", \"anio\", \"Nombre subdisciplina\") \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a782d0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sigla_equipo: string (nullable = true)\n",
      " |-- anio: integer (nullable = true)\n",
      " |-- Nombre subdisciplina: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "medallistaXAnio2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "524090e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+--------------+------------------+\n",
      "|sigla_equipo|anio|Total medallas| Medallas promedio|\n",
      "+------------+----+--------------+------------------+\n",
      "|         USA|2012|           121|1.9836065573770492|\n",
      "|         FRA|2006|            12|1.3333333333333333|\n",
      "|         BLR|2000|             9|               1.8|\n",
      "|         FIN|1988|            10|               2.5|\n",
      "|         KOR|2010|             3|               1.5|\n",
      "|         FRA|1948|            52|              2.08|\n",
      "|         GBR|2000|            30|1.5789473684210527|\n",
      "|         QAT|2012|             2|               1.0|\n",
      "|         JPN|1932|            11|               2.2|\n",
      "|         FRG|1994|             2|               1.0|\n",
      "|         NED|1972|             7|               1.4|\n",
      "|         GER|1932|            35|1.8421052631578947|\n",
      "|         NZL|1988|            11|             1.375|\n",
      "|         AUS|1972|            13|1.1818181818181819|\n",
      "|         BAH|2008|             2|               2.0|\n",
      "|         SWE|1968|            11|             1.375|\n",
      "|         KOR|1988|            43|2.0476190476190474|\n",
      "|         IRL|1948|             1|               1.0|\n",
      "|         IND|1928|             7|               7.0|\n",
      "|         YUG|1976|            10|3.3333333333333335|\n",
      "+------------+----+--------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Realizando funciones de agrupaciones para mostrar el total de medallas y promedio.\n",
    "medallistaXAnio2.groupBy(\"sigla_equipo\", \"anio\") \\\n",
    "    .agg(sum(\"count\").alias(\"Total medallas\"), \\\n",
    "        avg(\"count\").alias(\"Medallas promedio\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0898d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registrar Data Fraames como SQL para interactuar con SQL.\n",
    "resultadosDF.registerTempTable(\"resultado\")\n",
    "deportistaOlimpicoDF.registerTempTable(\"deportista\")\n",
    "paisesDF.registerTempTable(\"paises\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "46b48ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+------+----+------+----+---------+\n",
      "|deportista_id|              nombre|genero|edad|altura|peso|equipo_id|\n",
      "+-------------+--------------------+------+----+------+----+---------+\n",
      "|            1|           A Dijiang|     1|  24|   180|80.0|      199|\n",
      "|            2|            A Lamusi|     1|  23|   170|60.0|      199|\n",
      "|            3| Gunnar Nielsen Aaby|     1|  24|     0| 0.0|      273|\n",
      "|            4|Edgar Lindenau Aabye|     1|  34|     0| 0.0|      278|\n",
      "|            5|Christine Jacoba ...|     2|  21|   185|82.0|      705|\n",
      "+-------------+--------------------+------+----+------+----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consultando los Data Frame transformados en tablas con los nombres de (resultado, deportista, paises)\n",
    "sqlContext.sql(\"SELECT * FROM deportista\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cee51549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------------+--------+---------+\n",
      "|resultado_id|medalla|deportista_id|juego_id|evento_id|\n",
      "+------------+-------+-------------+--------+---------+\n",
      "|           1|     NA|            1|      39|        1|\n",
      "|           2|     NA|            2|      49|        2|\n",
      "|           3|     NA|            3|       7|        3|\n",
      "|           4|   Gold|            4|       2|        4|\n",
      "|           5|     NA|            5|      36|        5|\n",
      "+------------+-------+-------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT * FROM resultado\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9a6ba67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------+\n",
      "|pais_id|       nombre_equipo|sigla_equipo|\n",
      "+-------+--------------------+------------+\n",
      "|      1|         30. Februar|         AUT|\n",
      "|      2|A North American ...|         MEX|\n",
      "|      3|           Acipactli|         MEX|\n",
      "|      4|             Acturus|         ARG|\n",
      "|      5|         Afghanistan|         AFG|\n",
      "+-------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT * FROM paises\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "04924e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+------------+\n",
      "|medalla|nombre_equipo|sigla_equipo|\n",
      "+-------+-------------+------------+\n",
      "|   Gold|       Zambia|         ZAM|\n",
      "|   Gold|       Zambia|         ZAM|\n",
      "|   Gold|       Zambia|         ZAM|\n",
      "|   Gold|       Zambia|         ZAM|\n",
      "|   Gold|       Zambia|         ZAM|\n",
      "|   Gold|       Zambia|         ZAM|\n",
      "|   Gold|       Zambia|         ZAM|\n",
      "|   Gold|       Zambia|         ZAM|\n",
      "|   Gold|       Zambia|         ZAM|\n",
      "|   Gold|       Zambia|         ZAM|\n",
      "|   Gold|       Zambia|         ZAM|\n",
      "|   Gold|       Zambia|         ZAM|\n",
      "|   Gold|       Zambia|         ZAM|\n",
      "|   Gold|       Zambia|         ZAM|\n",
      "|   Gold|       Zambia|         ZAM|\n",
      "|   Gold|       Zambia|         ZAM|\n",
      "|   Gold|       Zambia|         ZAM|\n",
      "|   Gold|       Zambia|         ZAM|\n",
      "|   Gold|       Zambia|         ZAM|\n",
      "|   Gold|       Zambia|         ZAM|\n",
      "+-------+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Realizando consultas con SQL.\n",
    "sqlContext.sql(\"\"\"\n",
    "                SELECT medalla,nombre_equipo,sigla_equipo FROM resultado r\n",
    "                JOIN deportista d\n",
    "                ON r.deportista_id = d.equipo_id\n",
    "                JOIN paises p\n",
    "                ON p.pais_id = d.equipo_id\n",
    "                WHERE medalla <> \"NA\"\n",
    "                ORDER BY sigla_equipo DESC\n",
    "                \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "96e9bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generando RDD y con la funcion lambda separamos los valores por coma \",\"\n",
    "deportistaError = spark.textFile(path+\"deportistaError.csv\") \\\n",
    "    .map( lambda l : l.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a0388376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+------+----+------+----+---------+\n",
      "|deportista_id|              nombre|genero|edad|altura|peso|equipo_id|\n",
      "+-------------+--------------------+------+----+------+----+---------+\n",
      "|            1|           A Dijiang|     1|  24|   180|  80|      199|\n",
      "|            2|            A Lamusi|     1|  23|   170|  60|      199|\n",
      "|            3| Gunnar Nielsen Aaby|     1|  24|  null|null|      273|\n",
      "|            4|Edgar Lindenau Aabye|     1|  34|  null|null|      278|\n",
      "|            5|Christine Jacoba ...|     2|  21|   185|  82|      705|\n",
      "|            6|     Per Knut Aaland|     1|  31|   188|  75|     1096|\n",
      "|            7|        John Aalberg|     1|  31|   183|  72|     1096|\n",
      "|            8|\"Cornelia \"\"Cor\"\"...|     2|  18|   168|null|      705|\n",
      "|            9|    Antti Sami Aalto|     1|  26|   186|  96|      350|\n",
      "|           10|\"Einar Ferdinand ...|     1|  26|  null|null|      350|\n",
      "|           11|  Jorma Ilmari Aalto|     1|  22|   182|76.5|      350|\n",
      "|           12|   Jyri Tapani Aalto|     1|  31|   172|  70|      350|\n",
      "|           13|  Minna Maarit Aalto|     2|  30|   159|55.5|      350|\n",
      "|           14|Pirjo Hannele Aal...|     2|  32|   171|  65|      350|\n",
      "|           15|Arvo Ossian Aaltonen|     1|  22|  null|null|      350|\n",
      "|           16|Juhamatti Tapio A...|     1|  28|   184|  85|      350|\n",
      "|           17|Paavo Johannes Aa...|     1|  28|   175|  64|      350|\n",
      "|           18|Timo Antero Aaltonen|     1|  31|   189| 130|      350|\n",
      "|           19|Win Valdemar Aalt...|     1|  54|  null|null|      350|\n",
      "|           20|  Kjetil Andr Aamodt|     1|  20|   176|  85|      742|\n",
      "+-------------+--------------------+------+----+------+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos los encabezados con la funcion eliminaEncabezado.\n",
    "deportistaError  = deportistaError.mapPartitionsWithIndex(eliminaEncabezado)\n",
    "\n",
    "# Mapeando los valores de deportistaError\n",
    "deportistaError = deportistaError.map(lambda l : (\n",
    "l[0],\n",
    "l[1],\n",
    "l[2],\n",
    "l[3],\n",
    "l[4],\n",
    "l[5],\n",
    "l[6]))\n",
    "\n",
    "# Creando el Schema para los campos del RDD\n",
    "schemadeportistaError = StructType([\n",
    "    StructField(\"deportista_id\",StringType(),False),\n",
    "    StructField(\"nombre\",StringType(),False),\n",
    "    StructField(\"genero\",StringType(),False),\n",
    "    StructField(\"edad\",StringType(),False),\n",
    "    StructField(\"altura\",StringType(),False),\n",
    "    StructField(\"peso\",StringType(),False),\n",
    "    StructField(\"equipo_id\",StringType(),False)\n",
    "])\n",
    "\n",
    "# Creando Dataframe con los parametros de mapeo y schema.\n",
    "#deportistaErrorDF = sqlContext.createDataFrame(deportistaError, schemadeportistaError)\n",
    "deportistaErrorDF = sqlContext.read.schema(schemadeportistaError).option(\"header\",\"true\").csv(path + \"deportistaError.csv\")\n",
    "\n",
    "# Visualizando el dataframe con show().\n",
    "deportistaErrorDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a13da23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(z)>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generando funcion UDF y registrando la funcion.\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def conversionEnteros(valor):\n",
    "    return int(valor) if len(valor) > 0 else None\n",
    "\n",
    "conversionEnteros_udf = udf(lambda z: conversionEnteros(z),IntegerType())\n",
    "sqlContext.udf.register(\"conversionEnteros_udf\",conversionEnteros_udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d3cb4596",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o428.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 250) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:131)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:535)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:189)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 24 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\r\n\tat sun.reflect.GeneratedMethodAccessor70.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:131)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:535)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:189)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 24 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-b1f1fa3ba075>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdeportistaErrorDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconversionEnteros_udf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"altura\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \"\"\"\n\u001b[0;32m    483\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    485\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o428.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 250) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:131)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:535)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:189)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 24 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\r\n\tat sun.reflect.GeneratedMethodAccessor70.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:131)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:535)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:189)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 24 more\r\n"
     ]
    }
   ],
   "source": [
    "deportistaErrorDF.select(conversionEnteros_udf(\"altura\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e8000b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a602e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbe86aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419e6603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbbb756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81c526c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
